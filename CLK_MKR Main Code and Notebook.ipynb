{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLK_MKR.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pUghgXKmWNTX",
        "ZMbIQXhNZXGr"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg002y4jaVnr"
      },
      "source": [
        "# Run the following two cells to install the necessary packages and compile the functions needed for CLK-MKR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUghgXKmWNTX"
      },
      "source": [
        "## Dependencies and Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-E06tu_V0E3"
      },
      "source": [
        "#Basic\n",
        "import pandas as pd\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math \n",
        "\n",
        "#CV\n",
        "from sklearn.utils import safe_sqr\n",
        "from sklearn.base import clone\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "#Feature Select\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import RFECV, RFE\n",
        "from boruta import BorutaPy\n",
        "\n",
        "#Models\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.linear_model import ElasticNetCV, ElasticNet\n",
        "from sklearn.linear_model import LassoCV, Lasso\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4r2_uwuWTMA"
      },
      "source": [
        "## Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLWZuHoEWSVr"
      },
      "source": [
        "def vanilla(X, y):\n",
        "    elas = ElasticNet()\n",
        "    name = 'Vanilla de novo'\n",
        "    data = X\n",
        "    age = y\n",
        "    X = X.values\n",
        "    y = y.values\n",
        "    benchmark_elas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    result_dict = {}\n",
        "    cpgs_dict = {}\n",
        "    for i in benchmark_elas:\n",
        "        scores = []\n",
        "        cv = KFold(n_splits = 10, shuffle = True, random_state = 88)\n",
        "        for train_indices, test_indices in cv.split(X):\n",
        "            X_train, X_test = X[train_indices, :], X[test_indices, :]\n",
        "            y_train, y_test = y[train_indices], y[test_indices]\n",
        "            model = ElasticNet(alpha = i)\n",
        "            model.fit(X_train, y_train)\n",
        "            y_pred = model.predict(X_test)\n",
        "            acc = r2_score(y_test, y_pred)\n",
        "            scores.append(acc)\n",
        "        mean = np.mean(scores)\n",
        "\n",
        "        coef = pd.DataFrame({'coef': model.coef_, 'cpg' : data.columns})\n",
        "        cur = coef[coef.coef != 0]\n",
        "\n",
        "        result_dict[i] = mean\n",
        "        cpgs_dict[i] = list(cur.iloc[:,1])\n",
        "        \n",
        "    max_scoring_parameter = list(result_dict.keys())[list(result_dict.values()).index(max(result_dict.values()))]\n",
        "    selected_cpgs = cpgs_dict[max_scoring_parameter]\n",
        "    mean_score, std = reduced_dataset_training(data, age, selected_cpgs)\n",
        "    return(name, mean_score, selected_cpgs, std)\n",
        "\n",
        "def reduced_dataset_training(X, y, selected_cpgs):\n",
        "    if len(selected_cpgs) ==0:\n",
        "        return(0, 0)\n",
        "    else:\n",
        "      X = X[selected_cpgs].values\n",
        "      y = y.values\n",
        "\n",
        "      elas = ElasticNet()\n",
        "      chosen_model = elas\n",
        "      chosen_score = 0\n",
        "      score_list = []\n",
        "\n",
        "      outer_cv = KFold(n_splits = 10, shuffle = True, random_state = 88)\n",
        "\n",
        "      for train_indices, test_indices in outer_cv.split(X):\n",
        "          X_train, X_test = X[train_indices, :], X[test_indices, :]\n",
        "          y_train, y_test = y[train_indices], y[test_indices]\n",
        "\n",
        "          inner_cv = KFold(n_splits = 5, shuffle = True, random_state = 88)\n",
        "          model = elas\n",
        "          param_grid = {\"max_iter\": [100, 500, 1000],\n",
        "                    \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
        "                    \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
        "\n",
        "          grid = GridSearchCV(model, param_grid, scoring='r2', cv=inner_cv, refit=True)\n",
        "          result = grid.fit(X_train, y_train)\n",
        "          best_model = result.best_estimator_\n",
        "          y_pred = best_model.predict(X_test)\n",
        "\n",
        "          acc = r2_score(y_test, y_pred)\n",
        "          score_list.append(acc)\n",
        "          if acc> chosen_score:\n",
        "              chosen_score = acc\n",
        "              chosen_model = best_model\n",
        "\n",
        "      return(np.mean(score_list), np.std(score_list))\n",
        "\n",
        "\n",
        "def boruta(X, y):\n",
        "    name = 'Boruta de novo'\n",
        "    rf = RandomForestRegressor()\n",
        "    if len(X.columns) > 1500:\n",
        "        selector = BorutaPy(estimator = rf, n_estimators = 8, verbose=10).fit(np.array(X), np.array(y))\n",
        "        selected_cpgs = list(X.columns[selector.support_])\n",
        "        mean_score, std = reduced_dataset_training(X, y, selected_cpgs)        \n",
        "        return(name, mean_score, selected_cpgs, std)\n",
        "    \n",
        "    elif len(X.columns) <= 1500:\n",
        "        selector = BorutaPy(estimator = rf, n_estimators = 'auto', verbose=10).fit(np.array(X), np.array(y))\n",
        "        selected_cpgs = X.columns[selector.support_]\n",
        "        mean_score, std = reduced_dataset_training(X, y, selected_cpgs)\n",
        "        return(name, mean_score, selected_cpgs, std)\n",
        "\n",
        "\n",
        "def preselected_with_boruta(X, y, selected_cpgs, name):\n",
        "\n",
        "    origin = name\n",
        "    X = X[selected_cpgs]\n",
        "    if len(X.columns) <= 0:\n",
        "        return(origin, 0, [], 0)\n",
        "    else:\n",
        "        name, mean_score, new_cpgs, std = boruta(X,y)\n",
        "        return(origin, mean_score, new_cpgs, std)\n",
        "\n",
        "\n",
        "def SFM(X, y):\n",
        "    name = \"SFM de novo\"\n",
        "    thresh_list = [0.01, 0.05, 0.1, 0.5]\n",
        "    \n",
        "    best_score= 0\n",
        "    best_cpgs = []\n",
        "    best_std = 0\n",
        "    \n",
        "    for i in thresh_list:\n",
        "        print(\"Completing SFM with threshold: \" +str(i))\n",
        "        selector = SelectFromModel(elas, threshold=i).fit(X, y)\n",
        "        feature_idx = selector.get_support(indices=True)\n",
        "        selected_cpgs = X.columns[feature_idx]\n",
        "        \n",
        "        mean_score, std = reduced_dataset_training(X, y, selected_cpgs)\n",
        "        if mean_score > best_score:\n",
        "            best_cpgs = selected_cpgs\n",
        "            best_score = mean_score\n",
        "            best_std = best_std\n",
        "\n",
        "    return(name, best_score, best_cpgs, best_std)\n",
        "\n",
        "def RFE100(X, y, n_features_to_select = 100):\n",
        "    name = 'RFE de novo to 100'\n",
        "    elas = ElasticNet()\n",
        "    estimator = elas\n",
        "    n_features = X.shape[1]\n",
        "    n_features_to_select = n_features_to_select\n",
        "    support_ = np.ones(n_features, dtype=bool)\n",
        "    ranking_ = np.ones(n_features, dtype=int)\n",
        "    step = 0.01\n",
        "\n",
        "    while np.sum(support_) > n_features_to_select:\n",
        "        step = 0.01\n",
        "        features = np.arange(n_features)[support_]\n",
        "        estimator = clone(estimator)\n",
        "        print(\"Fitting estimator with %d features.\" % np.sum(support_))\n",
        "\n",
        "        estimator.fit(X.iloc[:,features], y)   \n",
        "        step = int(max(1, step * np.sum(support_)))\n",
        "        print(\"Eliminating \"+str(step)+ \" features\")\n",
        "\n",
        "        importances = estimator.coef_\n",
        "        if importances.ndim == 1:\n",
        "            importances = safe_sqr(importances)\n",
        "        else:\n",
        "            importances = safe_sqr(importances).sum(axis=0)\n",
        "\n",
        "        ranks = np.argsort(importances)\n",
        "        ranks = np.ravel(ranks)\n",
        "        threshold = min(step, np.sum(support_) - n_features_to_select)\n",
        "        support_[features[ranks][:threshold]] = False\n",
        "        ranking_[np.logical_not(support_)] += 1\n",
        "\n",
        "\n",
        "    features = np.arange(n_features)[support_]\n",
        "    estimator_ = clone(estimator)\n",
        "    final_model = estimator_.fit(X.iloc[:,features], y)\n",
        "\n",
        "    end_support = support_\n",
        "    end_ranking = ranking_\n",
        "    n_features_ = support_.sum()\n",
        "    feature_name = X.columns[end_support]\n",
        "    \n",
        "    selected_cpgs = list(feature_name)\n",
        "    mean_score, std = reduced_dataset_training(X, y, selected_cpgs)\n",
        "        \n",
        "    return(name, mean_score, selected_cpgs, std)  \n",
        "\n",
        "def RFE1500(X, y, n_features_to_select = 1500):\n",
        "    name = 'RFE de novo to 1500'\n",
        "    elas = ElasticNet()\n",
        "    estimator = elas\n",
        "    n_features = X.shape[1]\n",
        "    n_features_to_select = n_features_to_select\n",
        "    support_ = np.ones(n_features, dtype=bool)\n",
        "    ranking_ = np.ones(n_features, dtype=int)\n",
        "    step = 0.01\n",
        "\n",
        "    while np.sum(support_) > n_features_to_select:\n",
        "        step = 0.01\n",
        "        features = np.arange(n_features)[support_]\n",
        "        estimator = clone(estimator)\n",
        "        print(\"Fitting estimator with %d features.\" % np.sum(support_))\n",
        "\n",
        "        estimator.fit(X.iloc[:,features], y)   \n",
        "        step = int(max(1, step * np.sum(support_)))\n",
        "        print(\"Eliminating \"+str(step)+ \" features\")\n",
        "\n",
        "        importances = estimator.coef_\n",
        "        if importances.ndim == 1:\n",
        "            importances = safe_sqr(importances)\n",
        "        else:\n",
        "            importances = safe_sqr(importances).sum(axis=0)\n",
        "\n",
        "        ranks = np.argsort(importances)\n",
        "        ranks = np.ravel(ranks)\n",
        "        threshold = min(step, np.sum(support_) - n_features_to_select)\n",
        "        support_[features[ranks][:threshold]] = False\n",
        "        ranking_[np.logical_not(support_)] += 1\n",
        "\n",
        "\n",
        "    features = np.arange(n_features)[support_]\n",
        "    estimator_ = clone(estimator)\n",
        "    final_model = estimator_.fit(X.iloc[:,features], y)\n",
        "\n",
        "    end_support = support_\n",
        "    end_ranking = ranking_\n",
        "    n_features_ = support_.sum()\n",
        "    feature_name = X.columns[end_support]\n",
        "    \n",
        "    selected_cpgs = list(feature_name)\n",
        "    return(selected_cpgs)  \n",
        "    \n",
        "def training_intersected_cpgs(X,y,score_dict):\n",
        "    name = 'Intersection of all selected CpGs'\n",
        "    all_cpgs = []\n",
        "    for score in score_dict.keys():\n",
        "        all_cpgs.append(score_dict[score][1])\n",
        "    all_cpgs = list(pd.Series(all_cpgs).dropna())\n",
        "    intersected_cpgs = list(set([item for sublist in all_cpgs for item in sublist]))\n",
        "    mean_score, std = reduced_dataset_training(X, y, intersected_cpgs)\n",
        "    return(name, mean_score, intersected_cpgs, std)\n",
        "    \n",
        "def feature_selection(filepath):\n",
        "  input_data = pd.read_csv(filepath)\n",
        "  data = input_data.drop(['Age'], axis = 1)\n",
        "  age = input_data['Age']\n",
        "  score_dict = {}\n",
        "\n",
        "  #_____________________________________________________________________________________________________________________________________\n",
        "\n",
        "  vanilla_name, vanilla_score, vanilla_cpgs, vanilla_std = vanilla(data,age)\n",
        "  score_dict[vanilla_score] = [vanilla_name, vanilla_cpgs, vanilla_std]\n",
        "  #_____________________________________________________________________________________________________________________________________\n",
        "\n",
        "  boruta_name, boruta_mean_score, boruta_selected_cpgs, boruta_std= boruta(data,age)\n",
        "  score_dict[boruta_mean_score] = [boruta_name, boruta_selected_cpgs, boruta_std]\n",
        "\n",
        "  #_____________________________________________________________________________________________________________________________________\n",
        "\n",
        "  rfe_1500_cpgs = RFE1500(data,age)\n",
        "  rfe_1000_cpgs = RFE1500(data[rfe_1500_cpgs],age, 1000) #Use the 1500 before to go to 1000\n",
        "  elas = ElasticNet()\n",
        "  selector = RFECV(estimator = elas, step=1, cv = 10, scoring = 'r2').fit(data[rfe_1000_cpgs], age)\n",
        "  feature_idx = selector.get_support(indices=True)\n",
        "  rfe_1000_to_rfecv = list(data[rfe_1000_cpgs].columns[feature_idx])\n",
        "  rfe_1000_to_rfecv_score, rfe_1000_to_rfecv_std = reduced_dataset_training(data, age, rfe_1000_to_rfecv)\n",
        "  score_dict[rfe_1000_to_rfecv_score] = ['RFE de novo to 1000 followed by RFECV', rfe_1000_to_rfecv, rfe_1000_to_rfecv_std]\n",
        "\n",
        "  rfe_w_boruta_name, rfe_w_boruta_mean_score, rfe_w_boruta_selected_cpgs, rfe_w_boruta_std = preselected_with_boruta(\n",
        "      data, age, rfe_1500_cpgs, 'RFE de novo to 1500 followed by Boruta')\n",
        "  score_dict[rfe_w_boruta_mean_score] = [rfe_w_boruta_name, rfe_w_boruta_selected_cpgs, rfe_w_boruta_std]\n",
        "\n",
        "  rfe_name, rfe_mean_score, rfe_selected_cpgs, rfe_std = RFE100(data[rfe_1000_cpgs],age)\n",
        "  score_dict[rfe_mean_score] = [rfe_name, rfe_selected_cpgs, rfe_std]\n",
        "  #_____________________________________________________________________________________________________________________________________\n",
        "\n",
        "  sfm_name, sfm_mean_score, sfm_selected_cpgs, sfm_std = SFM(data,age)\n",
        "  score_dict[sfm_mean_score] = [sfm_name, sfm_selected_cpgs, sfm_std]\n",
        "\n",
        "  sfm_w_boruta_name, sfm_w_boruta_mean_score, sfm_w_boruta_selected_cpgs, sfm_w_boruta_std = preselected_with_boruta(\n",
        "      data,age, sfm_selected_cpgs, 'SFM de novo followed by Boruta')\n",
        "  score_dict[sfm_w_boruta_mean_score] = [sfm_w_boruta_name, sfm_w_boruta_selected_cpgs, sfm_w_boruta_std]\n",
        "\n",
        "  rfe_1000_to_sfm_name, rfe_1000_to_sfm_mean_score,  rfe_1000_to_sfm_selected_cpgs,  rfe_1000_to_sfm_std = SFM(data[rfe_1000_cpgs],age)\n",
        "  score_dict[rfe_1000_to_sfm_mean_score] = ['RFE de novo to 1000 followed by SFM', list(rfe_1000_to_sfm_selected_cpgs), rfe_1000_to_sfm_std]\n",
        "  #_____________________________________________________________________________________________________________________________________\n",
        "\n",
        "  inter_name, inter_mean_score, inter_selected_cpgs, inter_std = training_intersected_cpgs(data, age, score_dict)    \n",
        "  score_dict[inter_mean_score] = [inter_name, inter_selected_cpgs, inter_std]\n",
        "\n",
        "  inter_w_boruta_name, inter_w_boruta_mean_score, inter_w_boruta_selected_cpgs, inter_w_boruta_std = preselected_with_boruta(\n",
        "      data,age, inter_selected_cpgs, 'Intersection followed by Boruta')\n",
        "  score_dict[inter_w_boruta_mean_score] = [inter_w_boruta_name, inter_w_boruta_selected_cpgs, inter_w_boruta_std]\n",
        "\n",
        "  #_____________________________________________________________________________________________________________________________________\n",
        "\n",
        "  for score in score_dict.keys():\n",
        "      print(score_dict[score][0]+ \" selected \"+ str(len(score_dict[score][1])) \n",
        "            +\" CpGs. Mean R2 Score: \" + str(score) +\" \"+\"(\"+str(score_dict[score][2])+\")\")\n",
        "      \n",
        "  max_key = max(score_dict.keys())\n",
        "  score_dict[max_key][0] = score_dict[max_key][0]+\" (Best Model)\"\n",
        "  print(str(score_dict[max_key][0])+\" had the best score of \"+ str(max_key)+\" and selected \"\n",
        "  + str(len(score_dict[score][1])) +\" CpGs. CpG List saved. Clock Coefficients and Intercept of best performing model saved.\")\n",
        "\n",
        "  #Building a final clock\n",
        "\n",
        "  param_grid = {\"max_iter\": [100, 500, 1000],\n",
        "            \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
        "            \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n",
        "\n",
        "  X = data[score_dict[max_key][1]]\n",
        "  y = age\n",
        "\n",
        "  if len(X.columns) <= 0:\n",
        "      print('Feature selection was not successful and no viable CpGs were selected. No viable clock built.')\n",
        "      \n",
        "  else:\n",
        "      grid = GridSearchCV(estimator=elas,\n",
        "                          param_grid=param_grid,\n",
        "                          scoring='r2',\n",
        "                          cv=10,\n",
        "                          n_jobs=-1)\n",
        "      grid.fit(X, y)\n",
        "      best_parameters = grid.best_params_\n",
        "\n",
        "      final_clock_model = ElasticNet(alpha = best_parameters['alpha'], l1_ratio = best_parameters['l1_ratio'], max_iter = best_parameters['max_iter'])\n",
        "      X = data[score_dict[max_key][1]]\n",
        "      y = age\n",
        "      final_clock_model.fit(X,y)\n",
        "      pickle.dump(final_clock_model, open('final_clock_model.pkl', 'wb'))\n",
        "      final_intercept = final_clock_model.intercept_\n",
        "      final_coef = final_clock_model.coef_\n",
        "      y_pred = final_clock_model.predict(data[score_dict[max_key][1]])\n",
        "\n",
        "      print(str(score_dict[max_key][0])+\" had the best score of \"+ str(max_key)+\" and selected \"\n",
        "      + str(len(score_dict[max_key][1])) +\" CpGs. CpG List saved. Clock Coefficients and Intercept of best performing model saved.\")\n",
        "\n",
        "      cur_dict = {'CpGs' : score_dict[max_key][1], 'Coefficients': final_clock_model.coef_, 'Intercept': final_clock_model.intercept_, \n",
        "                'ElasticNet Alpha' : best_parameters['alpha'], 'ElasticNet L1 Ratio' : best_parameters['l1_ratio'], 'ElasticNet Max Iterations' : best_parameters['max_iter'] }\n",
        "      cur_df = pd.DataFrame(cur_dict)\n",
        "      cur_df.to_csv('best_model_cpgs_coefficients_intercept.csv',index=False)\n",
        "      \n",
        "      age_graph = pd.DataFrame({'Chronological Age' : age, 'Predicted Age' : y_pred})\n",
        "      age_labels = []\n",
        "      for index, row in age_graph.iterrows():\n",
        "          if row[0] <=20:\n",
        "              age_labels.append('Youth (1-20)')\n",
        "          elif row[0] >20 and row[0] <= 40:\n",
        "              age_labels.append('Adult (20-40)')\n",
        "          elif row[0] >40 and row[0] <= 60:\n",
        "              age_labels.append('MiddleAged (40-60)')\n",
        "          elif row[0] >60 and row[0] <= 80:\n",
        "              age_labels.append('Old (60-80)')\n",
        "          elif row[0] >80:\n",
        "              age_labels.append('Elderly (80+)')\n",
        "          else:\n",
        "              age_labels.append(float('nan'))\n",
        "      age_graph['Age Labels'] = age_labels\n",
        "      age_graph.to_csv('age_graph.csv',index=False)\n",
        "      \n",
        "      labelled_best_cpgs = data[score_dict[max_key][1]]\n",
        "      labelled_best_cpgs['Age'] = age\n",
        "      labelled_best_cpgs['Age Labels'] = age_labels\n",
        "      labelled_best_cpgs.to_csv('labelled_best_cpgs.csv', index=False)\n",
        "\n",
        "  final_cpgs_df = pd.DataFrame()\n",
        "  name_list = []\n",
        "  for score in score_dict.keys():\n",
        "      cur_cpg_list = pd.Series(score_dict[score][1])\n",
        "      name_list.append(score_dict[score][0])\n",
        "      final_cpgs_df = pd.concat([final_cpgs_df, cur_cpg_list], ignore_index=True, axis=1)\n",
        "  final_cpgs_df.columns = name_list \n",
        "  final_cpgs_df.columns = [col+ ' (' + str(round(score,5))+')' for col, score in zip(final_cpgs_df.columns, score_dict.keys())]\n",
        "      \n",
        "  final_cpgs_df.to_csv('final_cpg_list.csv', index=False)\n",
        "\n",
        "  print('Finished')\n",
        "  return('End')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMbIQXhNZXGr"
      },
      "source": [
        "# Run the main function below by entering the filepath as an argument.\n",
        "\n",
        "Example: feature_selection('methylation_data.csv')\n",
        "\n",
        "The feature selection and clock building will begin. This can take a while. You will see the results and 'Finished' at in the output cell once it is finished.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L66b3uSwbxJx"
      },
      "source": [
        "feature_selection(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58wqiCxRc5IY"
      },
      "source": [
        "# Results\n",
        "\n",
        "After the results are finished, you can run the following code to view the CpGs selected and their correlation with one another with Age Labels. You can also view visualizations of the clock's performance. \n",
        "\n",
        "Otherwise, the process is finished and you should have 5 new files created in the directory: \n",
        "\n",
        "1) 'final_cpg_list.csv' : Contains each set of CpGs selected with the name of the method used as columns  and the performance estimate it achieved as the header. The set that performed best will have ‘(Best Model)’ in brackets. The user can use this to see which CpGs are of importance and worth studying.\n",
        "\n",
        "2) 'final_clock_model.pkl' : A new model will be trained with the entirety of the dataset (no training/testing split) with the best performing set of features. Its hyperparameters will be optimized with GridsearchCV. The model is saved as a .PKL file. The user may now use it to predict the epigenetic age of any future samples.\n",
        "\n",
        "3) 'best_model_cpgs_coefficients_intercept.csv' : The model from 2) will also be saved in the form of coefficients, intercepts and corresponding CpG lists for the user to recreate themselves. It will also include the optimized hyperparameters for the ElasticNet (alpha, L1 ratio, maximum iterations).\n",
        "\n",
        "4) 'labelled_best_cpgs.csv' : The best model's CpGs and age but with an added column 'Age Labels' for higher level clustering of age groups. Used in the plotting sections below. \n",
        "\n",
        "5) 'age_graph.csv' : Table containing the chronological age of the original data and the predicted age that the best model produced when tested on the original data. Comes with 'Age Labels' also for plotting below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGwaAV0acPkj"
      },
      "source": [
        "The following code will show you the CpGs were selected in the best model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57lIsNXBUxHt"
      },
      "source": [
        "selected_cpgs_best_model = pd.read_csv('labelled_best_cpgs.csv')\n",
        "selected_cpgs_best_model.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgxXSVmwcN75"
      },
      "source": [
        "From the table above, you may freely choose any of the CpGs or age and enter them into the x or y variables below. They will then be plotted in the cell after."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHWKzniZVn-6"
      },
      "source": [
        "x = 'Age'\n",
        "y = 'cg08128734'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nyJW_6fcimU"
      },
      "source": [
        "Run the following cell to view the relationship between your chosen X and Y variables clustered by age group\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Wg07FzuVPK8"
      },
      "source": [
        "sns.set()\n",
        "sns.scatterplot(data=selected_cpgs_best_model, x=x, y=y, hue=\"Age Labels\")\n",
        "plt.xlabel(x)\n",
        "plt.ylabel(y)\n",
        "plt.title('CpG Methylation Values vs Age or other CpGs')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "sns.lmplot(data=selected_cpgs_best_model, x=x, y=y, hue=\"Age Labels\")\n",
        "plt.xlabel(x)\n",
        "plt.ylabel(y)\n",
        "plt.title('CpG Methylation Values vs Age or other CpGs')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw7NQERyVrLt"
      },
      "source": [
        "The following code will allow you to view the performance of the best model when its asked to predict on the original dataset. Run the following cell to view the relationship between the chronological and predicted (epigenetic) age clustered by age group\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEWDDh_Gc5--"
      },
      "source": [
        "age_graph = pd.read_csv('age_graph.csv')\n",
        "sns.set()\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.scatterplot(data=age_graph, x=\"Chronological Age\", y=\"Predicted Age\", hue=\"Age Labels\")\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('Chronological Age (Years)')\n",
        "plt.ylabel('Predicted Age (Years)')\n",
        "plt.title('Predicted Age of Best Model on Original Dataset vs Chronological Age')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "sns.lmplot(data=age_graph, x=\"Chronological Age\", y=\"Predicted Age\", hue=\"Age Labels\")\n",
        "plt.axis('scaled')\n",
        "plt.xlabel('Chronological Age (Years)')\n",
        "plt.ylabel('Predicted Age (Years)')\n",
        "plt.title('Predicted Age of Best Model on Original Dataset vs Chronological Age')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}